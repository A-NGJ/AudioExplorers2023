{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3rTGfXj23Wm8ifp8NCcXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-NGJ/AudioExplorers2023/blob/main/%20AudioExplorers2023_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11CFSzW6bTi2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Embedding, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "\n",
        "\n",
        "def resize_data(data):\n",
        "    resized_data = np.zeros((data.shape[0], 32, 96))\n",
        "    for i, img in enumerate(data):\n",
        "        resized_img = cv2.resize(img, (96, 32))\n",
        "        resized_data[i] = resized_img\n",
        "    return resized_data\n",
        "\n",
        "\n",
        "# Load data\n",
        "X = np.load('training.npy')\n",
        "y = np.load('training_labels.npy')\n",
        "\n",
        "# Preprocess input data\n",
        "X = resize_data(X)\n",
        "\n",
        "\n",
        "# Define model parameters\n",
        "max_length = X.shape[1]\n",
        "input_dim = X.shape[2]\n",
        "output_dim = len(np.unique(y))\n",
        "d_model = 96\n",
        "num_heads = 2\n",
        "dff = 128\n",
        "num_layers = 2\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = 5\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "\n",
        "# Define Transformer encoder layer\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        # print('Last Dense layer units:')\n",
        "        # print(self.ffn.layers[-1].units)\n",
        "        # print('d_model:')\n",
        "        # print(d_model)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.mha(x, x, x)  # Multi-head attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Add and normalize\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # Feed-forward neural network\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Add and normalize\n",
        "\n",
        "        return out2\n",
        "\n",
        "# Define input layer\n",
        "inputs = Input(shape=(max_length, input_dim))\n",
        "\n",
        "# Add Transformer encoder layers\n",
        "x = inputs\n",
        "for i in range(num_layers):\n",
        "    transformer_encoder = TransformerEncoder(d_model, num_heads, dff, dropout_rate)\n",
        "    x = transformer_encoder(x, training=True)\n",
        "\n",
        "# Flatten and add output layer\n",
        "x = Flatten()(x)\n",
        "outputs = Dense(output_dim, activation='softmax')(x)\n",
        "\n",
        "# Define and compile model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "#optimizer = Adam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[EarlyStopping(patience=4)])\n",
        "\n",
        "# model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[EarlyStopping(patience=2)])\n",
        "\n",
        "# Save the model\n",
        "model.save('music_detector_transformer3.h5')"
      ]
    }
  ]
}