{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfgNU214QWrcJ9lrfbMl8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-NGJ/AudioExplorers2023/blob/main/1DCNN_withEnsembleLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ0HO-xN5-VP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import mode\n",
        "\n",
        "def create_1d_cnn_leaky_model():\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "        tf.keras.layers.Reshape((32, 96), input_shape=(32, 96, 1)),\n",
        "        tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
        "        # tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "        # tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Conv1D(128, 3, activation='relu'),\n",
        "        # tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.01)),\n",
        "        # tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "X = np.load('training.npy')\n",
        "y = np.load('training_labels.npy')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1875, random_state=1) # 0.1875 x 0.85 = 0.15\n",
        "\n",
        "#X_train = np.load('X_train_augAJBig2.npy') Here load the augmented train data\n",
        "#y_train = np.load('y_train_augAJBig2.npy')\n",
        "\n",
        "y_valOrginal = y_val\n",
        "y_testOriginal = y_test\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
        "\n",
        "# Reshape the data to have only one channel (for 1D convolutions)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
        "\n",
        "# Train 5 individual models\n",
        "num_models = 2\n",
        "# for one augmented model: 84%, 5 augmented models: 87.97%\n",
        "models = [create_1d_cnn_leaky_model() for _ in range(num_models)]\n",
        "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training model {i + 1}\")\n",
        "    model.summary()\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping])\n",
        "    number = str(i+1)\n",
        "    model.save(number +'ensemble.h5')\n",
        "\n",
        "\n",
        "# Ensemble predictions using voting\n",
        "predictions = [model.predict(X_val) for model in models]\n",
        "predictions = np.stack(predictions)\n",
        "ensemble_predictions = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = np.mean(ensemble_predictions == y_valOrginal)\n",
        "print(f\"Ensemble accuracyval: {ensemble_accuracy}\")\n",
        "\n",
        "# Ensemble predictions using voting\n",
        "predictions = [model.predict(X_test) for model in models]\n",
        "predictions = np.stack(predictions)\n",
        "ensemble_predictions = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = np.mean(ensemble_predictions == y_testOriginal)\n",
        "print(f\"Ensemble accuracyTEST: {ensemble_accuracy}\")"
      ]
    }
  ]
}